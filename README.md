# Eora LLM Bot

Простой сервис на FastAPI + HTML/CSS/JS, который отвечает на вопросы клиентов, используя материалы с сайта `eora.ru`. В ответ вставляются ссылки на использованные источники в виде меток `[N]`.


## Требования
- Python 3.10+
- Доступ в интернет для загрузки кейсов и обращения к API

## Настройка API-ключа
Ключ передаётся через переменные окружения:
- `OPENROUTER_API_KEY` (или совместимый `OPENAI_API_KEY`)

Модель можно указать переменными:
- `OPENROUTER_MODEL` или `MODEL` (по умолчанию: `openai/gpt-4o-mini`)

Ключ можно получить здесь:
- https://openrouter.ai/openai/gpt-4o-mini/api

Для локальной запуска нужно создать `.env` рядом с `main.py`:
```
OPENROUTER_API_KEY=sk-or-...
```

## Установка и запуск
Из корня проекта:
```
python -m venv .venv
.\.venv\Scripts\python -m pip install --upgrade pip
.\.venv\Scripts\python -m pip install -r requirements.txt
.\.venv\Scripts\python -m uvicorn main:app --host 127.0.0.1 --port 8000 --reload
```
Откройте: http://127.0.0.1:8000/

## Пример вопроса
```
Что вы можете сделать для ритейлеров?
```
Бот сформирует краткий ответ и добавит метки источников `[1]`, `[2]`, ... со ссылками на соответствующие кейсы.

## Структура
```
main.py
srclist.txt
static/
  index.html
  styles.css
  app.js
requirements.txt
README.md
```

## Примечания
- Для снижения размеров промпта каждый документ ограничен ~5000 символами.
- Если модель/ключ недоступны, API вернёт 502 (`Upstream model request failed`).
- Источники подгружаются на старте. При первом запросе возможна ленивое дозагружение.
- CORS отключён (same-origin). Для кросс-доменных запросов настройте прокси или включите CORS явно.
- Безопасность: загрузка источников разрешена только по http/https; приватные/локальные адреса отбрасываются (защита от SSRF). Ответы фронтенд-рендеринга экранируются.
